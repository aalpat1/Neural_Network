{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# COMS 4995_002 Deep Learning Assignment 1\n",
    "Due on Monday, Oct 9, 11:59pm\n",
    "\n",
    "This assignment can be done in groups of at most 3 students. Everyone must submit on Courseworks individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the UNIs of your group (if applicable)\n",
    "\n",
    "Member 1: Name, UNI\n",
    "\n",
    "Member 2: Name, UNI\n",
    "\n",
    "Member 3: Name, UNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.num_layers =len(layer_dimensions)\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.train_cost = []\n",
    "        self.validation_cost = []\n",
    "        self.train_accuracy = []\n",
    "        self.validation_accuracy = []\n",
    "        \n",
    "        # init parameters\n",
    "        self.parameters = self.init_parameters(layer_dimensions)\n",
    "        \n",
    "    def init_parameters(self, layer_dimensions):\n",
    "        \n",
    "        parameters = {}\n",
    "        \n",
    "        for i in range(len(layer_dimensions) - 1):\n",
    "            forwoard_unit_number = layer_dimensions[i+1]\n",
    "            backwoard_unit_number = layer_dimensions[i]\n",
    "            parameters['W'+str(i+1)] = 0.001 * np.random.randn(forwoard_unit_number, backwoard_unit_number)\n",
    "            parameters['b'+str(i+1)] = np.zeros((forwoard_unit_number,1))\n",
    "            \n",
    "        return parameters\n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "        return np.dot(W, A) + b\n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        return relu(A)\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(0,X)\n",
    "            \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: \n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        M = np.random.rand(A.shape[0], A.shape[1])\n",
    "        M = 1*(M >= prob)\n",
    "        A = np.multiply(A, M)\n",
    "        A = A / (1 - prob)\n",
    "\n",
    "        return A, M\n",
    "\n",
    "    \n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        cache = {}\n",
    "        cache['A' + str(0)] = X\n",
    "        \n",
    "        for i in range(self.num_layers - 2):\n",
    "            W = self.parameters['W' + str(i+1)]\n",
    "            b = self.parameters['b' + str(i+1)]\n",
    "            A = cache['A' + str(i)]\n",
    "            Z = self.affineForward(A, W, b)\n",
    "            A_next = self.relu(Z)\n",
    "            cache['A' + str(i+1)] = A_next\n",
    "            cache['Z' + str(i+1)] = Z\n",
    "        \n",
    "        W = self.parameters['W' + str(self.num_layers - 1)]\n",
    "        b = self.parameters['b' + str(self.num_layers - 1)]\n",
    "        Z = self.affineForward(A_next, W, b)\n",
    "        cache['Z' + str(self.num_layers - 1)] = Z\n",
    "        AL = self.softmax(Z)\n",
    "\n",
    "        return AL, cache\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        Z = Z - np.amax(Z, axis=0)\n",
    "        Z = np.exp(Z)\n",
    "        sum_ = np.sum(Z,axis=0, keepdims=True)\n",
    "        return Z / sum_\n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute loss\n",
    "#         dAL = np.zeros((AL.shape[0], AL.shape[1]))\n",
    "        y_one_hot = np.zeros((AL.shape[0], AL.shape[1]))\n",
    "        \n",
    "        cost = 0\n",
    "        for i in range(len(y)):\n",
    "            cost += -np.log(AL[y[i], i]) / len(y)\n",
    "            y_one_hot[y[i], i] = 1\n",
    "        \n",
    "#         cost /= len(y)\n",
    "        if self.reg_lambda > 0:\n",
    "            # add regularization\n",
    "           pass\n",
    "        \n",
    "        dAL = AL - y_one_hot\n",
    "        \n",
    "        return cost, dAL\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache, layer_num):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        m = dA_prev.shape[1]\n",
    "        A = cache['A' + str(layer_num-1)]\n",
    "        W = self.parameters['W' + str(layer_num)]\n",
    "        dZ = self.activationBackward(dA_prev, cache, layer_num)\n",
    "        dW = 1 / m * np.dot(dZ, A.T)\n",
    "        db = 1 / m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA = np.dot(W.T, dZ)\n",
    "\n",
    "        return dA, dW, db\n",
    "\n",
    "    def activationBackward(self, dA, cache, layer_num, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        dZ = dA * self.relu_derivative(cache['Z' + str(layer_num)])\n",
    "        return dZ\n",
    "\n",
    "        \n",
    "    def relu_derivative(self, cached_x):\n",
    "        dx = 1*(cached_x > 0)\n",
    "        return dx\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "\n",
    "        return dA\n",
    "\n",
    "    def backPropagation(self, dAL, AL, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        \n",
    "        m = AL.shape[1]\n",
    "        A = cache['A' + str(self.num_layers - 2)]\n",
    "        W = self.parameters['W' + str(self.num_layers - 1)]\n",
    "        dZ = dAL\n",
    "        dW = 1 / m * np.dot(dZ, A.T)\n",
    "        db = 1 / m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA = np.dot(W.T, dZ)\n",
    "        \n",
    "        gradients['dW' + str(self.num_layers - 1)] = dW\n",
    "        gradients['db' + str(self.num_layers - 1)] = db\n",
    "        \n",
    "        for layer_num in range(self.num_layers - 2, 0, -1):\n",
    "            dA, dW, db = self.affineBackward(dA, cache, layer_num)\n",
    "            gradients['dW' + str(layer_num)] = dW\n",
    "            gradients['db' + str(layer_num)] = db\n",
    "            \n",
    "            if self.drop_prob > 0:\n",
    "                #call dropout_backward\n",
    "               pass\n",
    "        \n",
    "        if self.reg_lambda > 0:\n",
    "            # add gradients from L2 regularization to each dW\n",
    "            pass\n",
    "            \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "        for i in range(self.num_layers - 1):\n",
    "            W = self.parameters['W' + str(i+1)]\n",
    "            b = self.parameters['b' + str(i+1)]\n",
    "            \n",
    "            dW = gradients['dW' + str(i+1)]\n",
    "            db = gradients['db' + str(i+1)]\n",
    "\n",
    "            W -= alpha * dW\n",
    "            b -= alpha * db\n",
    "            self.parameters['W' + str(i+1)] = W\n",
    "            self.parameters['b' + str(i+1)] = b\n",
    "\n",
    "    def train(self, X, y,X_validation, y_validation, iters=1000, alpha=0.1, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(0, iters):\n",
    "            # get minibatch\n",
    "            X_batch, y_batch = self.get_batch(X, y, batch_size)\n",
    "            \n",
    "            # forward prop\n",
    "            AL, cache = self.forwardPropagation(X_batch)\n",
    "            \n",
    "            # compute loss\n",
    "            cost, dAL = self.costFunction(AL, y_batch)\n",
    "            \n",
    "            # compute gradients\n",
    "            gradients = self.backPropagation(dAL, AL, cache)\n",
    "            \n",
    "            # update weights and biases based on gradient\n",
    "            self.updateParameters(gradients, alpha)\n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                train_y_predict = self.predict(X_batch)\n",
    "                train_accuracy = np.sum((train_y_predict == y_batch) * 1) / len(y_batch)\n",
    "                self.train_cost.append(cost)\n",
    "                self.train_accuracy.append(train_accuracy)\n",
    "                \n",
    "                print('the training cost after %04d iteration is %8.6f:'%(i, cost))\n",
    "                print('the training accuracy after %04d iteration is %8.2f:'%(i, train_accuracy))\n",
    "                \n",
    "                AL, _ = self.forwardPropagation(X_validation)\n",
    "                validation_cost, _ = self.costFunction(AL, y_validation)\n",
    "                validation_y_predict = self.predict(X_validation)\n",
    "                validation_accuracy = np.sum((validation_y_predict == y_validation) * 1) / len(y_validation)\n",
    "                self.validation_cost.append(validation_cost)\n",
    "                self.validation_accuracy.append(validation_accuracy)\n",
    "                print('the validation cost after %04d iteration is %8.6f:'%(i, validation_cost))\n",
    "                print('the validation accuracy after %04d iteration is %8.2f:'%(i, validation_accuracy)) \n",
    "    \n",
    "                            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        AL,_ = self.forwardPropagation(X)\n",
    "        y_pred = np.argmax(AL, axis=0)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "        m = X.shape[1]\n",
    "        start_index = np.random.randint(0, m - batch_size)\n",
    "        X_batch = X[:, start_index:(start_index + batch_size)]\n",
    "        y_batch = y[start_index:(start_index + batch_size)]\n",
    "\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training cost after 0000 iteration is 2.302553:\n",
      "the training accuracy after 0000 iteration is     0.18:\n",
      "the validation cost after 0000 iteration is 2.302578:\n",
      "the validation accuracy after 0000 iteration is     0.09:\n",
      "the training cost after 0100 iteration is 2.302694:\n",
      "the training accuracy after 0100 iteration is     0.10:\n",
      "the validation cost after 0100 iteration is 2.302522:\n",
      "the validation accuracy after 0100 iteration is     0.12:\n",
      "the training cost after 0200 iteration is 2.302398:\n",
      "the training accuracy after 0200 iteration is     0.14:\n",
      "the validation cost after 0200 iteration is 2.302501:\n",
      "the validation accuracy after 0200 iteration is     0.10:\n",
      "the training cost after 0300 iteration is 2.302280:\n",
      "the training accuracy after 0300 iteration is     0.14:\n",
      "the validation cost after 0300 iteration is 2.302379:\n",
      "the validation accuracy after 0300 iteration is     0.12:\n",
      "the training cost after 0400 iteration is 2.302181:\n",
      "the training accuracy after 0400 iteration is     0.13:\n",
      "the validation cost after 0400 iteration is 2.301997:\n",
      "the validation accuracy after 0400 iteration is     0.14:\n",
      "the training cost after 0500 iteration is 2.301688:\n",
      "the training accuracy after 0500 iteration is     0.12:\n",
      "the validation cost after 0500 iteration is 2.301294:\n",
      "the validation accuracy after 0500 iteration is     0.11:\n",
      "the training cost after 0600 iteration is 2.300168:\n",
      "the training accuracy after 0600 iteration is     0.08:\n",
      "the validation cost after 0600 iteration is 2.299486:\n",
      "the validation accuracy after 0600 iteration is     0.11:\n",
      "the training cost after 0700 iteration is 2.294373:\n",
      "the training accuracy after 0700 iteration is     0.11:\n",
      "the validation cost after 0700 iteration is 2.296016:\n",
      "the validation accuracy after 0700 iteration is     0.10:\n",
      "the training cost after 0800 iteration is 2.289820:\n",
      "the training accuracy after 0800 iteration is     0.12:\n",
      "the validation cost after 0800 iteration is 2.290268:\n",
      "the validation accuracy after 0800 iteration is     0.11:\n",
      "the training cost after 0900 iteration is 2.268303:\n",
      "the training accuracy after 0900 iteration is     0.17:\n",
      "the validation cost after 0900 iteration is 2.280505:\n",
      "the validation accuracy after 0900 iteration is     0.11:\n",
      "the training cost after 1000 iteration is 2.288009:\n",
      "the training accuracy after 1000 iteration is     0.18:\n",
      "the validation cost after 1000 iteration is 2.265549:\n",
      "the validation accuracy after 1000 iteration is     0.14:\n",
      "the training cost after 1100 iteration is 2.273039:\n",
      "the training accuracy after 1100 iteration is     0.16:\n",
      "the validation cost after 1100 iteration is 2.245063:\n",
      "the validation accuracy after 1100 iteration is     0.16:\n",
      "the training cost after 1200 iteration is 2.254979:\n",
      "the training accuracy after 1200 iteration is     0.18:\n",
      "the validation cost after 1200 iteration is 2.219408:\n",
      "the validation accuracy after 1200 iteration is     0.17:\n",
      "the training cost after 1300 iteration is 2.162913:\n",
      "the training accuracy after 1300 iteration is     0.26:\n",
      "the validation cost after 1300 iteration is 2.189738:\n",
      "the validation accuracy after 1300 iteration is     0.17:\n",
      "the training cost after 1400 iteration is 2.158585:\n",
      "the training accuracy after 1400 iteration is     0.14:\n",
      "the validation cost after 1400 iteration is 2.162946:\n",
      "the validation accuracy after 1400 iteration is     0.17:\n",
      "the training cost after 1500 iteration is 2.115863:\n",
      "the training accuracy after 1500 iteration is     0.13:\n",
      "the validation cost after 1500 iteration is 2.138709:\n",
      "the validation accuracy after 1500 iteration is     0.18:\n",
      "the training cost after 1600 iteration is 2.132729:\n",
      "the training accuracy after 1600 iteration is     0.16:\n",
      "the validation cost after 1600 iteration is 2.120879:\n",
      "the validation accuracy after 1600 iteration is     0.17:\n",
      "the training cost after 1700 iteration is 2.109487:\n",
      "the training accuracy after 1700 iteration is     0.21:\n",
      "the validation cost after 1700 iteration is 2.106304:\n",
      "the validation accuracy after 1700 iteration is     0.19:\n",
      "the training cost after 1800 iteration is 2.114676:\n",
      "the training accuracy after 1800 iteration is     0.16:\n",
      "the validation cost after 1800 iteration is 2.096404:\n",
      "the validation accuracy after 1800 iteration is     0.18:\n",
      "the training cost after 1900 iteration is 2.154918:\n",
      "the training accuracy after 1900 iteration is     0.16:\n",
      "the validation cost after 1900 iteration is 2.089232:\n",
      "the validation accuracy after 1900 iteration is     0.19:\n",
      "the training cost after 2000 iteration is 2.071309:\n",
      "the training accuracy after 2000 iteration is     0.16:\n",
      "the validation cost after 2000 iteration is 2.080864:\n",
      "the validation accuracy after 2000 iteration is     0.19:\n",
      "the training cost after 2100 iteration is 2.049303:\n",
      "the training accuracy after 2100 iteration is     0.30:\n",
      "the validation cost after 2100 iteration is 2.071634:\n",
      "the validation accuracy after 2100 iteration is     0.20:\n",
      "the training cost after 2200 iteration is 1.982543:\n",
      "the training accuracy after 2200 iteration is     0.25:\n",
      "the validation cost after 2200 iteration is 2.063648:\n",
      "the validation accuracy after 2200 iteration is     0.19:\n",
      "the training cost after 2300 iteration is 2.020543:\n",
      "the training accuracy after 2300 iteration is     0.25:\n",
      "the validation cost after 2300 iteration is 2.060711:\n",
      "the validation accuracy after 2300 iteration is     0.22:\n",
      "the training cost after 2400 iteration is 2.066503:\n",
      "the training accuracy after 2400 iteration is     0.31:\n",
      "the validation cost after 2400 iteration is 2.050472:\n",
      "the validation accuracy after 2400 iteration is     0.22:\n",
      "the training cost after 2500 iteration is 2.087699:\n",
      "the training accuracy after 2500 iteration is     0.24:\n",
      "the validation cost after 2500 iteration is 2.042462:\n",
      "the validation accuracy after 2500 iteration is     0.23:\n",
      "the training cost after 2600 iteration is 2.001655:\n",
      "the training accuracy after 2600 iteration is     0.22:\n",
      "the validation cost after 2600 iteration is 2.034486:\n",
      "the validation accuracy after 2600 iteration is     0.24:\n",
      "the training cost after 2700 iteration is 2.039192:\n",
      "the training accuracy after 2700 iteration is     0.36:\n",
      "the validation cost after 2700 iteration is 2.027796:\n",
      "the validation accuracy after 2700 iteration is     0.24:\n",
      "the training cost after 2800 iteration is 1.979543:\n",
      "the training accuracy after 2800 iteration is     0.26:\n",
      "the validation cost after 2800 iteration is 2.017703:\n",
      "the validation accuracy after 2800 iteration is     0.25:\n",
      "the training cost after 2900 iteration is 2.006827:\n",
      "the training accuracy after 2900 iteration is     0.24:\n",
      "the validation cost after 2900 iteration is 2.010242:\n",
      "the validation accuracy after 2900 iteration is     0.25:\n",
      "the training cost after 3000 iteration is 2.114694:\n",
      "the training accuracy after 3000 iteration is     0.19:\n",
      "the validation cost after 3000 iteration is 2.001514:\n",
      "the validation accuracy after 3000 iteration is     0.25:\n",
      "the training cost after 3100 iteration is 1.909498:\n",
      "the training accuracy after 3100 iteration is     0.30:\n",
      "the validation cost after 3100 iteration is 1.992958:\n",
      "the validation accuracy after 3100 iteration is     0.25:\n",
      "the training cost after 3200 iteration is 1.918502:\n",
      "the training accuracy after 3200 iteration is     0.28:\n",
      "the validation cost after 3200 iteration is 1.986810:\n",
      "the validation accuracy after 3200 iteration is     0.26:\n",
      "the training cost after 3300 iteration is 1.925375:\n",
      "the training accuracy after 3300 iteration is     0.28:\n",
      "the validation cost after 3300 iteration is 1.979889:\n",
      "the validation accuracy after 3300 iteration is     0.26:\n",
      "the training cost after 3400 iteration is 1.946056:\n",
      "the training accuracy after 3400 iteration is     0.34:\n",
      "the validation cost after 3400 iteration is 1.974053:\n",
      "the validation accuracy after 3400 iteration is     0.27:\n",
      "the training cost after 3500 iteration is 2.031928:\n",
      "the training accuracy after 3500 iteration is     0.33:\n",
      "the validation cost after 3500 iteration is 1.966278:\n",
      "the validation accuracy after 3500 iteration is     0.27:\n",
      "the training cost after 3600 iteration is 2.088458:\n",
      "the training accuracy after 3600 iteration is     0.20:\n",
      "the validation cost after 3600 iteration is 1.963787:\n",
      "the validation accuracy after 3600 iteration is     0.27:\n",
      "the training cost after 3700 iteration is 2.023443:\n",
      "the training accuracy after 3700 iteration is     0.24:\n",
      "the validation cost after 3700 iteration is 1.958429:\n",
      "the validation accuracy after 3700 iteration is     0.28:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training cost after 3800 iteration is 2.003230:\n",
      "the training accuracy after 3800 iteration is     0.39:\n",
      "the validation cost after 3800 iteration is 1.949629:\n",
      "the validation accuracy after 3800 iteration is     0.28:\n",
      "the training cost after 3900 iteration is 1.974031:\n",
      "the training accuracy after 3900 iteration is     0.19:\n",
      "the validation cost after 3900 iteration is 1.945098:\n",
      "the validation accuracy after 3900 iteration is     0.28:\n",
      "the training cost after 4000 iteration is 1.991586:\n",
      "the training accuracy after 4000 iteration is     0.23:\n",
      "the validation cost after 4000 iteration is 1.938872:\n",
      "the validation accuracy after 4000 iteration is     0.29:\n",
      "the training cost after 4100 iteration is 1.962410:\n",
      "the training accuracy after 4100 iteration is     0.31:\n",
      "the validation cost after 4100 iteration is 1.937796:\n",
      "the validation accuracy after 4100 iteration is     0.28:\n",
      "the training cost after 4200 iteration is 1.982642:\n",
      "the training accuracy after 4200 iteration is     0.29:\n",
      "the validation cost after 4200 iteration is 1.933839:\n",
      "the validation accuracy after 4200 iteration is     0.29:\n",
      "the training cost after 4300 iteration is 2.063865:\n",
      "the training accuracy after 4300 iteration is     0.22:\n",
      "the validation cost after 4300 iteration is 1.926607:\n",
      "the validation accuracy after 4300 iteration is     0.30:\n",
      "the training cost after 4400 iteration is 1.911294:\n",
      "the training accuracy after 4400 iteration is     0.23:\n",
      "the validation cost after 4400 iteration is 1.924424:\n",
      "the validation accuracy after 4400 iteration is     0.30:\n",
      "the training cost after 4500 iteration is 1.932633:\n",
      "the training accuracy after 4500 iteration is     0.35:\n",
      "the validation cost after 4500 iteration is 1.919619:\n",
      "the validation accuracy after 4500 iteration is     0.30:\n",
      "the training cost after 4600 iteration is 1.772164:\n",
      "the training accuracy after 4600 iteration is     0.39:\n",
      "the validation cost after 4600 iteration is 1.919310:\n",
      "the validation accuracy after 4600 iteration is     0.30:\n",
      "the training cost after 4700 iteration is 1.962932:\n",
      "the training accuracy after 4700 iteration is     0.31:\n",
      "the validation cost after 4700 iteration is 1.914926:\n",
      "the validation accuracy after 4700 iteration is     0.30:\n",
      "the training cost after 4800 iteration is 1.859813:\n",
      "the training accuracy after 4800 iteration is     0.34:\n",
      "the validation cost after 4800 iteration is 1.912277:\n",
      "the validation accuracy after 4800 iteration is     0.31:\n",
      "the training cost after 4900 iteration is 1.823721:\n",
      "the training accuracy after 4900 iteration is     0.35:\n",
      "the validation cost after 4900 iteration is 1.907838:\n",
      "the validation accuracy after 4900 iteration is     0.31:\n",
      "the training cost after 5000 iteration is 1.861412:\n",
      "the training accuracy after 5000 iteration is     0.28:\n",
      "the validation cost after 5000 iteration is 1.905086:\n",
      "the validation accuracy after 5000 iteration is     0.31:\n",
      "the training cost after 5100 iteration is 1.742744:\n",
      "the training accuracy after 5100 iteration is     0.41:\n",
      "the validation cost after 5100 iteration is 1.902549:\n",
      "the validation accuracy after 5100 iteration is     0.31:\n",
      "the training cost after 5200 iteration is 1.988612:\n",
      "the training accuracy after 5200 iteration is     0.33:\n",
      "the validation cost after 5200 iteration is 1.897550:\n",
      "the validation accuracy after 5200 iteration is     0.31:\n",
      "the training cost after 5300 iteration is 1.874697:\n",
      "the training accuracy after 5300 iteration is     0.25:\n",
      "the validation cost after 5300 iteration is 1.896619:\n",
      "the validation accuracy after 5300 iteration is     0.31:\n",
      "the training cost after 5400 iteration is 1.895722:\n",
      "the training accuracy after 5400 iteration is     0.33:\n",
      "the validation cost after 5400 iteration is 1.891806:\n",
      "the validation accuracy after 5400 iteration is     0.31:\n",
      "the training cost after 5500 iteration is 1.915085:\n",
      "the training accuracy after 5500 iteration is     0.32:\n",
      "the validation cost after 5500 iteration is 1.889627:\n",
      "the validation accuracy after 5500 iteration is     0.31:\n",
      "the training cost after 5600 iteration is 2.084344:\n",
      "the training accuracy after 5600 iteration is     0.40:\n",
      "the validation cost after 5600 iteration is 1.885618:\n",
      "the validation accuracy after 5600 iteration is     0.32:\n",
      "the training cost after 5700 iteration is 2.018853:\n",
      "the training accuracy after 5700 iteration is     0.26:\n",
      "the validation cost after 5700 iteration is 1.882328:\n",
      "the validation accuracy after 5700 iteration is     0.32:\n",
      "the training cost after 5800 iteration is 1.903799:\n",
      "the training accuracy after 5800 iteration is     0.34:\n",
      "the validation cost after 5800 iteration is 1.877071:\n",
      "the validation accuracy after 5800 iteration is     0.33:\n",
      "the training cost after 5900 iteration is 1.970739:\n",
      "the training accuracy after 5900 iteration is     0.34:\n",
      "the validation cost after 5900 iteration is 1.873769:\n",
      "the validation accuracy after 5900 iteration is     0.32:\n",
      "the training cost after 6000 iteration is 1.782096:\n",
      "the training accuracy after 6000 iteration is     0.38:\n",
      "the validation cost after 6000 iteration is 1.869336:\n",
      "the validation accuracy after 6000 iteration is     0.33:\n",
      "the training cost after 6100 iteration is 1.902862:\n",
      "the training accuracy after 6100 iteration is     0.33:\n",
      "the validation cost after 6100 iteration is 1.868584:\n",
      "the validation accuracy after 6100 iteration is     0.33:\n",
      "the training cost after 6200 iteration is 1.773440:\n",
      "the training accuracy after 6200 iteration is     0.45:\n",
      "the validation cost after 6200 iteration is 1.865898:\n",
      "the validation accuracy after 6200 iteration is     0.32:\n",
      "the training cost after 6300 iteration is 1.791226:\n",
      "the training accuracy after 6300 iteration is     0.36:\n",
      "the validation cost after 6300 iteration is 1.862205:\n",
      "the validation accuracy after 6300 iteration is     0.33:\n",
      "the training cost after 6400 iteration is 1.931237:\n",
      "the training accuracy after 6400 iteration is     0.34:\n",
      "the validation cost after 6400 iteration is 1.855949:\n",
      "the validation accuracy after 6400 iteration is     0.33:\n",
      "the training cost after 6500 iteration is 1.765747:\n",
      "the training accuracy after 6500 iteration is     0.34:\n",
      "the validation cost after 6500 iteration is 1.852947:\n",
      "the validation accuracy after 6500 iteration is     0.33:\n",
      "the training cost after 6600 iteration is 1.786307:\n",
      "the training accuracy after 6600 iteration is     0.36:\n",
      "the validation cost after 6600 iteration is 1.850965:\n",
      "the validation accuracy after 6600 iteration is     0.34:\n",
      "the training cost after 6700 iteration is 1.789736:\n",
      "the training accuracy after 6700 iteration is     0.39:\n",
      "the validation cost after 6700 iteration is 1.848851:\n",
      "the validation accuracy after 6700 iteration is     0.34:\n",
      "the training cost after 6800 iteration is 2.012202:\n",
      "the training accuracy after 6800 iteration is     0.30:\n",
      "the validation cost after 6800 iteration is 1.844124:\n",
      "the validation accuracy after 6800 iteration is     0.34:\n",
      "the training cost after 6900 iteration is 1.808897:\n",
      "the training accuracy after 6900 iteration is     0.31:\n",
      "the validation cost after 6900 iteration is 1.838606:\n",
      "the validation accuracy after 6900 iteration is     0.34:\n",
      "the training cost after 7000 iteration is 1.781586:\n",
      "the training accuracy after 7000 iteration is     0.35:\n",
      "the validation cost after 7000 iteration is 1.839378:\n",
      "the validation accuracy after 7000 iteration is     0.34:\n",
      "the training cost after 7100 iteration is 1.780936:\n",
      "the training accuracy after 7100 iteration is     0.36:\n",
      "the validation cost after 7100 iteration is 1.833271:\n",
      "the validation accuracy after 7100 iteration is     0.34:\n",
      "the training cost after 7200 iteration is 1.796471:\n",
      "the training accuracy after 7200 iteration is     0.29:\n",
      "the validation cost after 7200 iteration is 1.828576:\n",
      "the validation accuracy after 7200 iteration is     0.35:\n",
      "the training cost after 7300 iteration is 1.812867:\n",
      "the training accuracy after 7300 iteration is     0.34:\n",
      "the validation cost after 7300 iteration is 1.827108:\n",
      "the validation accuracy after 7300 iteration is     0.34:\n",
      "the training cost after 7400 iteration is 1.826666:\n",
      "the training accuracy after 7400 iteration is     0.35:\n",
      "the validation cost after 7400 iteration is 1.823935:\n",
      "the validation accuracy after 7400 iteration is     0.34:\n",
      "the training cost after 7500 iteration is 1.751056:\n",
      "the training accuracy after 7500 iteration is     0.41:\n",
      "the validation cost after 7500 iteration is 1.821245:\n",
      "the validation accuracy after 7500 iteration is     0.35:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training cost after 7600 iteration is 1.661816:\n",
      "the training accuracy after 7600 iteration is     0.44:\n",
      "the validation cost after 7600 iteration is 1.820005:\n",
      "the validation accuracy after 7600 iteration is     0.35:\n",
      "the training cost after 7700 iteration is 1.776571:\n",
      "the training accuracy after 7700 iteration is     0.42:\n",
      "the validation cost after 7700 iteration is 1.814257:\n",
      "the validation accuracy after 7700 iteration is     0.36:\n",
      "the training cost after 7800 iteration is 1.861388:\n",
      "the training accuracy after 7800 iteration is     0.31:\n",
      "the validation cost after 7800 iteration is 1.812496:\n",
      "the validation accuracy after 7800 iteration is     0.35:\n",
      "the training cost after 7900 iteration is 1.749683:\n",
      "the training accuracy after 7900 iteration is     0.35:\n",
      "the validation cost after 7900 iteration is 1.810129:\n",
      "the validation accuracy after 7900 iteration is     0.36:\n",
      "the training cost after 8000 iteration is 1.766756:\n",
      "the training accuracy after 8000 iteration is     0.39:\n",
      "the validation cost after 8000 iteration is 1.804457:\n",
      "the validation accuracy after 8000 iteration is     0.37:\n",
      "the training cost after 8100 iteration is 1.823788:\n",
      "the training accuracy after 8100 iteration is     0.32:\n",
      "the validation cost after 8100 iteration is 1.804330:\n",
      "the validation accuracy after 8100 iteration is     0.36:\n",
      "the training cost after 8200 iteration is 1.712747:\n",
      "the training accuracy after 8200 iteration is     0.36:\n",
      "the validation cost after 8200 iteration is 1.800893:\n",
      "the validation accuracy after 8200 iteration is     0.36:\n",
      "the training cost after 8300 iteration is 1.734458:\n",
      "the training accuracy after 8300 iteration is     0.43:\n",
      "the validation cost after 8300 iteration is 1.802025:\n",
      "the validation accuracy after 8300 iteration is     0.36:\n",
      "the training cost after 8400 iteration is 1.884800:\n",
      "the training accuracy after 8400 iteration is     0.42:\n",
      "the validation cost after 8400 iteration is 1.796450:\n",
      "the validation accuracy after 8400 iteration is     0.36:\n",
      "the training cost after 8500 iteration is 1.839270:\n",
      "the training accuracy after 8500 iteration is     0.31:\n",
      "the validation cost after 8500 iteration is 1.793487:\n",
      "the validation accuracy after 8500 iteration is     0.36:\n",
      "the training cost after 8600 iteration is 1.844248:\n",
      "the training accuracy after 8600 iteration is     0.39:\n",
      "the validation cost after 8600 iteration is 1.790092:\n",
      "the validation accuracy after 8600 iteration is     0.37:\n",
      "the training cost after 8700 iteration is 1.764245:\n",
      "the training accuracy after 8700 iteration is     0.41:\n",
      "the validation cost after 8700 iteration is 1.794792:\n",
      "the validation accuracy after 8700 iteration is     0.36:\n",
      "the training cost after 8800 iteration is 1.837743:\n",
      "the training accuracy after 8800 iteration is     0.37:\n",
      "the validation cost after 8800 iteration is 1.784952:\n",
      "the validation accuracy after 8800 iteration is     0.37:\n",
      "the training cost after 8900 iteration is 1.729665:\n",
      "the training accuracy after 8900 iteration is     0.44:\n",
      "the validation cost after 8900 iteration is 1.787738:\n",
      "the validation accuracy after 8900 iteration is     0.36:\n",
      "the training cost after 9000 iteration is 1.717266:\n",
      "the training accuracy after 9000 iteration is     0.46:\n",
      "the validation cost after 9000 iteration is 1.782514:\n",
      "the validation accuracy after 9000 iteration is     0.37:\n",
      "the training cost after 9100 iteration is 1.753492:\n",
      "the training accuracy after 9100 iteration is     0.34:\n",
      "the validation cost after 9100 iteration is 1.778740:\n",
      "the validation accuracy after 9100 iteration is     0.37:\n",
      "the training cost after 9200 iteration is 1.820647:\n",
      "the training accuracy after 9200 iteration is     0.37:\n",
      "the validation cost after 9200 iteration is 1.777758:\n",
      "the validation accuracy after 9200 iteration is     0.37:\n",
      "the training cost after 9300 iteration is 1.642607:\n",
      "the training accuracy after 9300 iteration is     0.42:\n",
      "the validation cost after 9300 iteration is 1.774241:\n",
      "the validation accuracy after 9300 iteration is     0.37:\n",
      "the training cost after 9400 iteration is 1.803713:\n",
      "the training accuracy after 9400 iteration is     0.38:\n",
      "the validation cost after 9400 iteration is 1.781131:\n",
      "the validation accuracy after 9400 iteration is     0.37:\n",
      "the training cost after 9500 iteration is 1.786403:\n",
      "the training accuracy after 9500 iteration is     0.40:\n",
      "the validation cost after 9500 iteration is 1.774851:\n",
      "the validation accuracy after 9500 iteration is     0.37:\n",
      "the training cost after 9600 iteration is 1.757565:\n",
      "the training accuracy after 9600 iteration is     0.42:\n",
      "the validation cost after 9600 iteration is 1.769893:\n",
      "the validation accuracy after 9600 iteration is     0.37:\n",
      "the training cost after 9700 iteration is 1.814955:\n",
      "the training accuracy after 9700 iteration is     0.39:\n",
      "the validation cost after 9700 iteration is 1.767559:\n",
      "the validation accuracy after 9700 iteration is     0.38:\n",
      "the training cost after 9800 iteration is 1.691516:\n",
      "the training accuracy after 9800 iteration is     0.45:\n",
      "the validation cost after 9800 iteration is 1.761089:\n",
      "the validation accuracy after 9800 iteration is     0.38:\n",
      "the training cost after 9900 iteration is 1.644573:\n",
      "the training accuracy after 9900 iteration is     0.46:\n",
      "the validation cost after 9900 iteration is 1.763034:\n",
      "the validation accuracy after 9900 iteration is     0.37:\n",
      "the training cost after 10000 iteration is 1.813596:\n",
      "the training accuracy after 10000 iteration is     0.39:\n",
      "the validation cost after 10000 iteration is 1.760769:\n",
      "the validation accuracy after 10000 iteration is     0.38:\n",
      "the training cost after 10100 iteration is 1.800067:\n",
      "the training accuracy after 10100 iteration is     0.37:\n",
      "the validation cost after 10100 iteration is 1.755676:\n",
      "the validation accuracy after 10100 iteration is     0.38:\n",
      "the training cost after 10200 iteration is 1.857791:\n",
      "the training accuracy after 10200 iteration is     0.40:\n",
      "the validation cost after 10200 iteration is 1.756515:\n",
      "the validation accuracy after 10200 iteration is     0.38:\n",
      "the training cost after 10300 iteration is 1.725824:\n",
      "the training accuracy after 10300 iteration is     0.38:\n",
      "the validation cost after 10300 iteration is 1.753411:\n",
      "the validation accuracy after 10300 iteration is     0.38:\n",
      "the training cost after 10400 iteration is 1.924603:\n",
      "the training accuracy after 10400 iteration is     0.35:\n",
      "the validation cost after 10400 iteration is 1.749111:\n",
      "the validation accuracy after 10400 iteration is     0.39:\n",
      "the training cost after 10500 iteration is 1.714637:\n",
      "the training accuracy after 10500 iteration is     0.37:\n",
      "the validation cost after 10500 iteration is 1.748411:\n",
      "the validation accuracy after 10500 iteration is     0.39:\n",
      "the training cost after 10600 iteration is 1.761480:\n",
      "the training accuracy after 10600 iteration is     0.39:\n",
      "the validation cost after 10600 iteration is 1.747004:\n",
      "the validation accuracy after 10600 iteration is     0.38:\n",
      "the training cost after 10700 iteration is 1.669535:\n",
      "the training accuracy after 10700 iteration is     0.40:\n",
      "the validation cost after 10700 iteration is 1.746471:\n",
      "the validation accuracy after 10700 iteration is     0.38:\n",
      "the training cost after 10800 iteration is 1.625342:\n",
      "the training accuracy after 10800 iteration is     0.39:\n",
      "the validation cost after 10800 iteration is 1.742505:\n",
      "the validation accuracy after 10800 iteration is     0.39:\n",
      "the training cost after 10900 iteration is 1.765068:\n",
      "the training accuracy after 10900 iteration is     0.42:\n",
      "the validation cost after 10900 iteration is 1.740804:\n",
      "the validation accuracy after 10900 iteration is     0.39:\n",
      "the training cost after 11000 iteration is 1.736958:\n",
      "the training accuracy after 11000 iteration is     0.39:\n",
      "the validation cost after 11000 iteration is 1.738189:\n",
      "the validation accuracy after 11000 iteration is     0.39:\n",
      "the training cost after 11100 iteration is 1.604787:\n",
      "the training accuracy after 11100 iteration is     0.40:\n",
      "the validation cost after 11100 iteration is 1.740167:\n",
      "the validation accuracy after 11100 iteration is     0.39:\n",
      "the training cost after 11200 iteration is 1.802365:\n",
      "the training accuracy after 11200 iteration is     0.40:\n",
      "the validation cost after 11200 iteration is 1.735225:\n",
      "the validation accuracy after 11200 iteration is     0.39:\n",
      "the training cost after 11300 iteration is 1.747733:\n",
      "the training accuracy after 11300 iteration is     0.32:\n",
      "the validation cost after 11300 iteration is 1.732437:\n",
      "the validation accuracy after 11300 iteration is     0.39:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training cost after 11400 iteration is 1.715339:\n",
      "the training accuracy after 11400 iteration is     0.40:\n",
      "the validation cost after 11400 iteration is 1.739463:\n",
      "the validation accuracy after 11400 iteration is     0.39:\n",
      "the training cost after 11500 iteration is 1.719556:\n",
      "the training accuracy after 11500 iteration is     0.35:\n",
      "the validation cost after 11500 iteration is 1.726797:\n",
      "the validation accuracy after 11500 iteration is     0.39:\n",
      "the training cost after 11600 iteration is 1.871446:\n",
      "the training accuracy after 11600 iteration is     0.38:\n",
      "the validation cost after 11600 iteration is 1.732336:\n",
      "the validation accuracy after 11600 iteration is     0.39:\n",
      "the training cost after 11700 iteration is 1.774245:\n",
      "the training accuracy after 11700 iteration is     0.32:\n",
      "the validation cost after 11700 iteration is 1.723574:\n",
      "the validation accuracy after 11700 iteration is     0.40:\n",
      "the training cost after 11800 iteration is 1.693415:\n",
      "the training accuracy after 11800 iteration is     0.32:\n",
      "the validation cost after 11800 iteration is 1.726269:\n",
      "the validation accuracy after 11800 iteration is     0.39:\n",
      "the training cost after 11900 iteration is 1.613593:\n",
      "the training accuracy after 11900 iteration is     0.49:\n",
      "the validation cost after 11900 iteration is 1.724472:\n",
      "the validation accuracy after 11900 iteration is     0.39:\n",
      "the training cost after 12000 iteration is 1.712372:\n",
      "the training accuracy after 12000 iteration is     0.42:\n",
      "the validation cost after 12000 iteration is 1.721901:\n",
      "the validation accuracy after 12000 iteration is     0.39:\n",
      "the training cost after 12100 iteration is 1.777036:\n",
      "the training accuracy after 12100 iteration is     0.44:\n",
      "the validation cost after 12100 iteration is 1.721745:\n",
      "the validation accuracy after 12100 iteration is     0.40:\n",
      "the training cost after 12200 iteration is 1.690725:\n",
      "the training accuracy after 12200 iteration is     0.37:\n",
      "the validation cost after 12200 iteration is 1.718088:\n",
      "the validation accuracy after 12200 iteration is     0.40:\n",
      "the training cost after 12300 iteration is 1.561974:\n",
      "the training accuracy after 12300 iteration is     0.47:\n",
      "the validation cost after 12300 iteration is 1.719421:\n",
      "the validation accuracy after 12300 iteration is     0.39:\n",
      "the training cost after 12400 iteration is 1.668095:\n",
      "the training accuracy after 12400 iteration is     0.40:\n",
      "the validation cost after 12400 iteration is 1.712662:\n",
      "the validation accuracy after 12400 iteration is     0.40:\n",
      "the training cost after 12500 iteration is 1.519686:\n",
      "the training accuracy after 12500 iteration is     0.45:\n",
      "the validation cost after 12500 iteration is 1.711214:\n",
      "the validation accuracy after 12500 iteration is     0.39:\n",
      "the training cost after 12600 iteration is 1.809051:\n",
      "the training accuracy after 12600 iteration is     0.37:\n",
      "the validation cost after 12600 iteration is 1.709579:\n",
      "the validation accuracy after 12600 iteration is     0.40:\n",
      "the training cost after 12700 iteration is 1.659787:\n",
      "the training accuracy after 12700 iteration is     0.40:\n",
      "the validation cost after 12700 iteration is 1.710051:\n",
      "the validation accuracy after 12700 iteration is     0.40:\n",
      "the training cost after 12800 iteration is 1.643800:\n",
      "the training accuracy after 12800 iteration is     0.44:\n",
      "the validation cost after 12800 iteration is 1.707823:\n",
      "the validation accuracy after 12800 iteration is     0.40:\n",
      "the training cost after 12900 iteration is 1.779281:\n",
      "the training accuracy after 12900 iteration is     0.43:\n",
      "the validation cost after 12900 iteration is 1.705388:\n",
      "the validation accuracy after 12900 iteration is     0.39:\n",
      "the training cost after 13000 iteration is 1.770512:\n",
      "the training accuracy after 13000 iteration is     0.37:\n",
      "the validation cost after 13000 iteration is 1.703931:\n",
      "the validation accuracy after 13000 iteration is     0.40:\n",
      "the training cost after 13100 iteration is 1.635532:\n",
      "the training accuracy after 13100 iteration is     0.44:\n",
      "the validation cost after 13100 iteration is 1.706833:\n",
      "the validation accuracy after 13100 iteration is     0.40:\n",
      "the training cost after 13200 iteration is 1.767438:\n",
      "the training accuracy after 13200 iteration is     0.36:\n",
      "the validation cost after 13200 iteration is 1.707269:\n",
      "the validation accuracy after 13200 iteration is     0.40:\n",
      "the training cost after 13300 iteration is 1.764397:\n",
      "the training accuracy after 13300 iteration is     0.44:\n",
      "the validation cost after 13300 iteration is 1.702383:\n",
      "the validation accuracy after 13300 iteration is     0.40:\n",
      "the training cost after 13400 iteration is 1.652823:\n",
      "the training accuracy after 13400 iteration is     0.38:\n",
      "the validation cost after 13400 iteration is 1.700461:\n",
      "the validation accuracy after 13400 iteration is     0.40:\n",
      "the training cost after 13500 iteration is 1.776887:\n",
      "the training accuracy after 13500 iteration is     0.35:\n",
      "the validation cost after 13500 iteration is 1.698652:\n",
      "the validation accuracy after 13500 iteration is     0.40:\n",
      "the training cost after 13600 iteration is 1.621993:\n",
      "the training accuracy after 13600 iteration is     0.37:\n",
      "the validation cost after 13600 iteration is 1.695705:\n",
      "the validation accuracy after 13600 iteration is     0.40:\n",
      "the training cost after 13700 iteration is 1.741026:\n",
      "the training accuracy after 13700 iteration is     0.42:\n",
      "the validation cost after 13700 iteration is 1.693298:\n",
      "the validation accuracy after 13700 iteration is     0.41:\n",
      "the training cost after 13800 iteration is 1.942354:\n",
      "the training accuracy after 13800 iteration is     0.37:\n",
      "the validation cost after 13800 iteration is 1.690281:\n",
      "the validation accuracy after 13800 iteration is     0.41:\n",
      "the training cost after 13900 iteration is 1.687611:\n",
      "the training accuracy after 13900 iteration is     0.45:\n",
      "the validation cost after 13900 iteration is 1.693206:\n",
      "the validation accuracy after 13900 iteration is     0.41:\n",
      "the training cost after 14000 iteration is 1.800368:\n",
      "the training accuracy after 14000 iteration is     0.42:\n",
      "the validation cost after 14000 iteration is 1.687290:\n",
      "the validation accuracy after 14000 iteration is     0.41:\n",
      "the training cost after 14100 iteration is 1.768987:\n",
      "the training accuracy after 14100 iteration is     0.39:\n",
      "the validation cost after 14100 iteration is 1.686728:\n",
      "the validation accuracy after 14100 iteration is     0.41:\n",
      "the training cost after 14200 iteration is 1.729507:\n",
      "the training accuracy after 14200 iteration is     0.38:\n",
      "the validation cost after 14200 iteration is 1.685985:\n",
      "the validation accuracy after 14200 iteration is     0.41:\n",
      "the training cost after 14300 iteration is 1.539756:\n",
      "the training accuracy after 14300 iteration is     0.49:\n",
      "the validation cost after 14300 iteration is 1.694820:\n",
      "the validation accuracy after 14300 iteration is     0.40:\n",
      "the training cost after 14400 iteration is 1.748178:\n",
      "the training accuracy after 14400 iteration is     0.41:\n",
      "the validation cost after 14400 iteration is 1.686115:\n",
      "the validation accuracy after 14400 iteration is     0.41:\n",
      "the training cost after 14500 iteration is 1.715650:\n",
      "the training accuracy after 14500 iteration is     0.39:\n",
      "the validation cost after 14500 iteration is 1.683716:\n",
      "the validation accuracy after 14500 iteration is     0.41:\n",
      "the training cost after 14600 iteration is 1.618408:\n",
      "the training accuracy after 14600 iteration is     0.41:\n",
      "the validation cost after 14600 iteration is 1.685948:\n",
      "the validation accuracy after 14600 iteration is     0.41:\n",
      "the training cost after 14700 iteration is 1.680774:\n",
      "the training accuracy after 14700 iteration is     0.42:\n",
      "the validation cost after 14700 iteration is 1.689019:\n",
      "the validation accuracy after 14700 iteration is     0.40:\n",
      "the training cost after 14800 iteration is 1.519066:\n",
      "the training accuracy after 14800 iteration is     0.52:\n",
      "the validation cost after 14800 iteration is 1.679851:\n",
      "the validation accuracy after 14800 iteration is     0.41:\n",
      "the training cost after 14900 iteration is 1.646345:\n",
      "the training accuracy after 14900 iteration is     0.43:\n",
      "the validation cost after 14900 iteration is 1.689786:\n",
      "the validation accuracy after 14900 iteration is     0.41:\n",
      "the training cost after 15000 iteration is 1.665189:\n",
      "the training accuracy after 15000 iteration is     0.41:\n",
      "the validation cost after 15000 iteration is 1.685886:\n",
      "the validation accuracy after 15000 iteration is     0.41:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training cost after 15100 iteration is 1.596968:\n",
      "the training accuracy after 15100 iteration is     0.45:\n",
      "the validation cost after 15100 iteration is 1.673461:\n",
      "the validation accuracy after 15100 iteration is     0.41:\n",
      "the training cost after 15200 iteration is 1.587950:\n",
      "the training accuracy after 15200 iteration is     0.46:\n",
      "the validation cost after 15200 iteration is 1.674789:\n",
      "the validation accuracy after 15200 iteration is     0.41:\n",
      "the training cost after 15300 iteration is 1.537871:\n",
      "the training accuracy after 15300 iteration is     0.48:\n",
      "the validation cost after 15300 iteration is 1.675024:\n",
      "the validation accuracy after 15300 iteration is     0.41:\n",
      "the training cost after 15400 iteration is 1.767290:\n",
      "the training accuracy after 15400 iteration is     0.41:\n",
      "the validation cost after 15400 iteration is 1.670027:\n",
      "the validation accuracy after 15400 iteration is     0.41:\n",
      "the training cost after 15500 iteration is 1.589071:\n",
      "the training accuracy after 15500 iteration is     0.49:\n",
      "the validation cost after 15500 iteration is 1.669560:\n",
      "the validation accuracy after 15500 iteration is     0.42:\n",
      "the training cost after 15600 iteration is 1.477083:\n",
      "the training accuracy after 15600 iteration is     0.55:\n",
      "the validation cost after 15600 iteration is 1.666701:\n",
      "the validation accuracy after 15600 iteration is     0.41:\n",
      "the training cost after 15700 iteration is 1.963540:\n",
      "the training accuracy after 15700 iteration is     0.32:\n",
      "the validation cost after 15700 iteration is 1.666230:\n",
      "the validation accuracy after 15700 iteration is     0.41:\n",
      "the training cost after 15800 iteration is 1.757634:\n",
      "the training accuracy after 15800 iteration is     0.44:\n",
      "the validation cost after 15800 iteration is 1.678807:\n",
      "the validation accuracy after 15800 iteration is     0.40:\n",
      "the training cost after 15900 iteration is 1.579584:\n",
      "the training accuracy after 15900 iteration is     0.46:\n",
      "the validation cost after 15900 iteration is 1.663427:\n",
      "the validation accuracy after 15900 iteration is     0.42:\n",
      "the training cost after 16000 iteration is 1.791467:\n",
      "the training accuracy after 16000 iteration is     0.39:\n",
      "the validation cost after 16000 iteration is 1.661209:\n",
      "the validation accuracy after 16000 iteration is     0.42:\n",
      "the training cost after 16100 iteration is 1.588145:\n",
      "the training accuracy after 16100 iteration is     0.43:\n",
      "the validation cost after 16100 iteration is 1.664173:\n",
      "the validation accuracy after 16100 iteration is     0.42:\n",
      "the training cost after 16200 iteration is 1.632961:\n",
      "the training accuracy after 16200 iteration is     0.45:\n",
      "the validation cost after 16200 iteration is 1.662966:\n",
      "the validation accuracy after 16200 iteration is     0.42:\n",
      "the training cost after 16300 iteration is 1.628109:\n",
      "the training accuracy after 16300 iteration is     0.46:\n",
      "the validation cost after 16300 iteration is 1.660583:\n",
      "the validation accuracy after 16300 iteration is     0.42:\n",
      "the training cost after 16400 iteration is 1.615574:\n",
      "the training accuracy after 16400 iteration is     0.42:\n",
      "the validation cost after 16400 iteration is 1.666121:\n",
      "the validation accuracy after 16400 iteration is     0.41:\n",
      "the training cost after 16500 iteration is 1.713331:\n",
      "the training accuracy after 16500 iteration is     0.45:\n",
      "the validation cost after 16500 iteration is 1.662844:\n",
      "the validation accuracy after 16500 iteration is     0.42:\n",
      "the training cost after 16600 iteration is 1.852680:\n",
      "the training accuracy after 16600 iteration is     0.33:\n",
      "the validation cost after 16600 iteration is 1.664387:\n",
      "the validation accuracy after 16600 iteration is     0.42:\n",
      "the training cost after 16700 iteration is 1.649442:\n",
      "the training accuracy after 16700 iteration is     0.47:\n",
      "the validation cost after 16700 iteration is 1.656323:\n",
      "the validation accuracy after 16700 iteration is     0.42:\n",
      "the training cost after 16800 iteration is 1.717009:\n",
      "the training accuracy after 16800 iteration is     0.39:\n",
      "the validation cost after 16800 iteration is 1.654845:\n",
      "the validation accuracy after 16800 iteration is     0.42:\n",
      "the training cost after 16900 iteration is 1.549814:\n",
      "the training accuracy after 16900 iteration is     0.45:\n",
      "the validation cost after 16900 iteration is 1.649963:\n",
      "the validation accuracy after 16900 iteration is     0.42:\n",
      "the training cost after 17000 iteration is 1.519483:\n",
      "the training accuracy after 17000 iteration is     0.47:\n",
      "the validation cost after 17000 iteration is 1.651464:\n",
      "the validation accuracy after 17000 iteration is     0.42:\n",
      "the training cost after 17100 iteration is 1.424268:\n",
      "the training accuracy after 17100 iteration is     0.54:\n",
      "the validation cost after 17100 iteration is 1.650646:\n",
      "the validation accuracy after 17100 iteration is     0.42:\n",
      "the training cost after 17200 iteration is 1.532340:\n",
      "the training accuracy after 17200 iteration is     0.57:\n",
      "the validation cost after 17200 iteration is 1.650028:\n",
      "the validation accuracy after 17200 iteration is     0.42:\n",
      "the training cost after 17300 iteration is 1.478172:\n",
      "the training accuracy after 17300 iteration is     0.51:\n",
      "the validation cost after 17300 iteration is 1.650004:\n",
      "the validation accuracy after 17300 iteration is     0.42:\n",
      "the training cost after 17400 iteration is 1.662047:\n",
      "the training accuracy after 17400 iteration is     0.41:\n",
      "the validation cost after 17400 iteration is 1.652896:\n",
      "the validation accuracy after 17400 iteration is     0.42:\n",
      "the training cost after 17500 iteration is 1.706193:\n",
      "the training accuracy after 17500 iteration is     0.46:\n",
      "the validation cost after 17500 iteration is 1.650559:\n",
      "the validation accuracy after 17500 iteration is     0.42:\n",
      "the training cost after 17600 iteration is 1.787526:\n",
      "the training accuracy after 17600 iteration is     0.38:\n",
      "the validation cost after 17600 iteration is 1.646035:\n",
      "the validation accuracy after 17600 iteration is     0.42:\n",
      "the training cost after 17700 iteration is 1.808731:\n",
      "the training accuracy after 17700 iteration is     0.32:\n",
      "the validation cost after 17700 iteration is 1.645215:\n",
      "the validation accuracy after 17700 iteration is     0.42:\n",
      "the training cost after 17800 iteration is 1.524985:\n",
      "the training accuracy after 17800 iteration is     0.52:\n",
      "the validation cost after 17800 iteration is 1.648078:\n",
      "the validation accuracy after 17800 iteration is     0.42:\n",
      "the training cost after 17900 iteration is 1.668580:\n",
      "the training accuracy after 17900 iteration is     0.39:\n",
      "the validation cost after 17900 iteration is 1.643957:\n",
      "the validation accuracy after 17900 iteration is     0.43:\n",
      "the training cost after 18000 iteration is 1.481644:\n",
      "the training accuracy after 18000 iteration is     0.51:\n",
      "the validation cost after 18000 iteration is 1.641199:\n",
      "the validation accuracy after 18000 iteration is     0.43:\n",
      "the training cost after 18100 iteration is 1.599994:\n",
      "the training accuracy after 18100 iteration is     0.48:\n",
      "the validation cost after 18100 iteration is 1.651607:\n",
      "the validation accuracy after 18100 iteration is     0.42:\n",
      "the training cost after 18200 iteration is 1.609929:\n",
      "the training accuracy after 18200 iteration is     0.46:\n",
      "the validation cost after 18200 iteration is 1.643281:\n",
      "the validation accuracy after 18200 iteration is     0.42:\n",
      "the training cost after 18300 iteration is 1.705123:\n",
      "the training accuracy after 18300 iteration is     0.38:\n",
      "the validation cost after 18300 iteration is 1.636596:\n",
      "the validation accuracy after 18300 iteration is     0.43:\n",
      "the training cost after 18400 iteration is 1.796004:\n",
      "the training accuracy after 18400 iteration is     0.39:\n",
      "the validation cost after 18400 iteration is 1.634227:\n",
      "the validation accuracy after 18400 iteration is     0.43:\n",
      "the training cost after 18500 iteration is 1.608012:\n",
      "the training accuracy after 18500 iteration is     0.46:\n",
      "the validation cost after 18500 iteration is 1.634427:\n",
      "the validation accuracy after 18500 iteration is     0.43:\n",
      "the training cost after 18600 iteration is 1.645359:\n",
      "the training accuracy after 18600 iteration is     0.37:\n",
      "the validation cost after 18600 iteration is 1.638141:\n",
      "the validation accuracy after 18600 iteration is     0.42:\n",
      "the training cost after 18700 iteration is 1.690863:\n",
      "the training accuracy after 18700 iteration is     0.38:\n",
      "the validation cost after 18700 iteration is 1.629813:\n",
      "the validation accuracy after 18700 iteration is     0.43:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training cost after 18800 iteration is 1.660988:\n",
      "the training accuracy after 18800 iteration is     0.47:\n",
      "the validation cost after 18800 iteration is 1.638237:\n",
      "the validation accuracy after 18800 iteration is     0.43:\n",
      "the training cost after 18900 iteration is 1.650701:\n",
      "the training accuracy after 18900 iteration is     0.44:\n",
      "the validation cost after 18900 iteration is 1.632354:\n",
      "the validation accuracy after 18900 iteration is     0.43:\n",
      "the training cost after 19000 iteration is 1.570847:\n",
      "the training accuracy after 19000 iteration is     0.44:\n",
      "the validation cost after 19000 iteration is 1.630042:\n",
      "the validation accuracy after 19000 iteration is     0.43:\n",
      "the training cost after 19100 iteration is 1.611554:\n",
      "the training accuracy after 19100 iteration is     0.45:\n",
      "the validation cost after 19100 iteration is 1.627858:\n",
      "the validation accuracy after 19100 iteration is     0.43:\n",
      "the training cost after 19200 iteration is 1.554885:\n",
      "the training accuracy after 19200 iteration is     0.42:\n",
      "the validation cost after 19200 iteration is 1.628282:\n",
      "the validation accuracy after 19200 iteration is     0.43:\n",
      "the training cost after 19300 iteration is 1.710895:\n",
      "the training accuracy after 19300 iteration is     0.33:\n",
      "the validation cost after 19300 iteration is 1.627617:\n",
      "the validation accuracy after 19300 iteration is     0.43:\n",
      "the training cost after 19400 iteration is 1.611497:\n",
      "the training accuracy after 19400 iteration is     0.43:\n",
      "the validation cost after 19400 iteration is 1.628952:\n",
      "the validation accuracy after 19400 iteration is     0.43:\n",
      "the training cost after 19500 iteration is 1.474995:\n",
      "the training accuracy after 19500 iteration is     0.51:\n",
      "the validation cost after 19500 iteration is 1.628657:\n",
      "the validation accuracy after 19500 iteration is     0.43:\n",
      "the training cost after 19600 iteration is 1.501435:\n",
      "the training accuracy after 19600 iteration is     0.49:\n",
      "the validation cost after 19600 iteration is 1.624948:\n",
      "the validation accuracy after 19600 iteration is     0.43:\n",
      "the training cost after 19700 iteration is 1.569956:\n",
      "the training accuracy after 19700 iteration is     0.47:\n",
      "the validation cost after 19700 iteration is 1.634402:\n",
      "the validation accuracy after 19700 iteration is     0.42:\n",
      "the training cost after 19800 iteration is 1.617357:\n",
      "the training accuracy after 19800 iteration is     0.42:\n",
      "the validation cost after 19800 iteration is 1.623892:\n",
      "the validation accuracy after 19800 iteration is     0.43:\n",
      "the training cost after 19900 iteration is 1.601460:\n",
      "the training accuracy after 19900 iteration is     0.46:\n",
      "the validation cost after 19900 iteration is 1.620983:\n",
      "the validation accuracy after 19900 iteration is     0.43:\n"
     ]
    }
   ],
   "source": [
    "# try train, you can adjust your hyperparameter here, and plot the it on the next block\n",
    "layer_dimensions = [X_train.shape[0], 32, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, X_validation, y_validation,iters=20000, alpha=0.005, batch_size=100, print_every=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114842278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH9SA1JSXoiSZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18252298d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy on the validation set is:0.4282\n"
     ]
    }
   ],
   "source": [
    "# plot the result here\n",
    "plt.title(\"cost\")\n",
    "plt.plot(NN.train_cost,'r--', NN.validation_cost, 'bs')\n",
    "plt.show()\n",
    "plt.title(\"accuracy\")\n",
    "plt.plot(NN.train_accuracy,'r--', NN.validation_accuracy, 'bs')\n",
    "plt.show()\n",
    "y_predict_validation = NN.predict(X_validation)\n",
    "accuracy = np.sum((y_predict_validation == y_validation) * 1) / len(y_validation)\n",
    "print('the accuracy on the validation set is:' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = '/Users/zhengyixing/Documents/study/3nd_semester/DL/homework/cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')\n",
    "indexs = np.random.choice(50000, 5000, replace=False)\n",
    "X_validation = X_train[:,indexs]\n",
    "y_validation = y_train[indexs]\n",
    "X_train = np.delete(X_train,indexs, axis = 1)\n",
    "y_train = np.delete(y_train,indexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 45000)\n",
      "(45000,)\n",
      "[6 1 6 ..., 5 5 3]\n",
      "45000\n",
      "(3072, 10000)\n",
      "(3072, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train)\n",
    "print(len(y_train))\n",
    "print(X_test.shape)\n",
    "print(X_validation.shape)\n",
    "# try train \n",
    "# layer_dimensions = [X_train.shape[0], 32, 10]  # including the input and output layers\n",
    "# NN = NeuralNetwork(layer_dimensions)\n",
    "# NN.train(X_train, y_train, iters=1000, alpha=0.0001, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "#### Simple fully-connected deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer_dimensions = [X_train.shape[0], ..., 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=, alpha=, batch_size=, print_every=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted = NN.predict(X_test)\n",
    "save_predictions('ans1-uni', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-uni.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Regularizing the neural network\n",
    "#### Add dropout and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN2 = NeuralNetwork(layer_dimensions, drop_prob=0, reg_lambda=0)\n",
    "NN2.train(X_train, y_train, iters=1000, alpha=0.00001, batch_size=1000, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted2 = NN2.predict(X)\n",
    "save_predictions(y_predicted, 'ans2-uni')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
